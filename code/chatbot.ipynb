{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai langchain-openai langchain-community langchainhub langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import  RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import os\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from neo4j import  Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaFunctions(model=\"llama3.1\", temperature=0, format=\"json\")\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"You are an AI assistant design to tell funny jokes. Do not answer any question that is not a joke.\"),\n",
    "    HumanMessage(\"Tell me a joke.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"Prince\"\n",
    "txt = f\"Hello {username}\"\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me historical fact about the {event} in {location}.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.format(event=\"World War II\", location=\"Europe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_str = \"\"\"You are a helpful AI assistant. Given the \n",
    "following context, answer the question. If the answer can not be \n",
    "found in the context, simply say you DO NOT KNOW.\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_promt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=system_message_str,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_str = \"Can you provide details on:  {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=human_message_str,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [system_message_promt, human_message_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_prompt_template = ChatPromptTemplate(\n",
    "    messages=messages,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of India?\"\n",
    "context = \"India is a country in Asia. It's capital is Delhi.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_prompt_template.format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\n",
    "    chatbot_prompt_template.format(context=context, question=\"What is the capital of Kenya?\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chatbot_prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\n",
    "    \"context\": context,\n",
    "    \"question\": question\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\n",
    "    \"context\": context,\n",
    "    \"question\": \"What is the capital of India?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import (\n",
    "    AgentExecutor,\n",
    "    create_openai_tools_agent\n",
    ")\n",
    "from langchain.tools import tool\n",
    "from langchain_core.tools import ToolException\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_type = Literal[\"add\", \"multiply\", \"substraction\", \"division\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"Calculator-tool\", return_direct=False)\n",
    "def calculator(operator: operator_type, a: float, b: float) -> float:\n",
    "    \"\"\"A simple calculator tool that can add, multiply, substract, or divide two numbers.\"\"\"\n",
    "    if operator == \"add\":\n",
    "        return a + b\n",
    "    elif operator == \"multiply\":\n",
    "        return a * b\n",
    "    elif operator == \"substraction\":\n",
    "        return a - b\n",
    "    elif operator == \"division\":\n",
    "        return a / b\n",
    "    else:\n",
    "        raise ToolException(\"Invalid operator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculator.name)\n",
    "print(calculator.description)\n",
    "print(calculator.return_direct)\n",
    "print(calculator.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator.run({\"operator\": \"add\", \"a\": 2, \"b\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [calculator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_openai_tools_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\n",
    "    \"input\": \"What is the product of 2 and 3?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_graph_vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding = OllamaEmbeddings(\n",
    "    model=\"llama3.1\",\n",
    "),\n",
    "    index_name=\"product\",\n",
    "    node_label=\"Product\",\n",
    "    url=os.getenv(\"NEO4J_URL\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    text_node_properties=[\n",
    "        \"images\",\n",
    "        \"gender\",\n",
    "        \"price\",\n",
    "        \"name\",\n",
    "        \"mpn\"\n",
    "        \"currency\",\n",
    "        \"sku\",\n",
    "        \"in_stock\"\n",
    "    ],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neo4j_graph_vector_index.similarity_search(\"Parx\" , k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neo4j_graph_vector_index.similarity_search(\n",
    "    \"Product detail\", filter={\"brand\": \"DYNK\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in result:\n",
    "    clearned_page_content = dict(line.split(\": \", 1) for line in product.page_content.strip().split(\"\\n\"))\n",
    "    \n",
    "    print(f\"\"\"\n",
    "          Name: {clearned_page_content.get('name')}\n",
    "          Price: {clearned_page_content.get('price')}\n",
    "          Gender: {clearned_page_content.get('gender')}\n",
    "          Images: {clearned_page_content.get('images')}\n",
    "          In_stock: {product.metadata.get('in_stock')}\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_details_chat_template_str = \"\"\"\n",
    "Your job is to use the provided product data to answer \n",
    "questions about the description, brand, price and quality\n",
    "to the E-commerce customer. Use the following context to answer questions. \n",
    "Be as detailed as possible, but don't make up any information that's \n",
    "not from the context. If you don't know an answer, say you don't know.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_details_chat_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=product_details_chat_template_str\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"Can you provide details on: {question}?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [product_details_chat_system_prompt, human_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate(\n",
    "    messages=messages,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaFunctions(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=neo4j_graph_vector_index.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_chain.combine_documents_chain.llm_chain.prompt = qa_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chain.invoke(\"Give me details of DYNK products from database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.get(\"result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URL\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_generation_template = \"\"\"\n",
    "Task:\n",
    "Generate Cypher query for a Neo4j graph database.\n",
    "\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Note:\n",
    "Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything other than\n",
    "for you to construct a Cypher statement. Do not include any text except\n",
    "the generated Cypher statement. Make sure the direction of the relationship is\n",
    "correct in your queries. Make sure you alias both entities and relationships\n",
    "properly. Do not run any queries that would add to or delete from\n",
    "the database. Make sure to alias all statements that follow as with\n",
    "statement (e.g. WITH c as customer, o.orderID as order_id).\n",
    "If you need to divide numbers, make sure to\n",
    "filter the denominator to be non-zero.\n",
    "\n",
    "Examples:\n",
    "# Retrieve the total number of orders placed by each customer.\n",
    "MATCH (c:Customer)-[o:ORDERED_BY]->(order:Order)\n",
    "RETURN c.customerID AS customer_id, COUNT(o) AS total_orders\n",
    "# List the top 5 products with the highest unit price.\n",
    "MATCH (p:Product)\n",
    "RETURN p.productName AS product_name, p.price AS product_price\n",
    "ORDER BY product_price DESC\n",
    "LIMIT 5\n",
    "# Find all products which is sold most.\n",
    "MATCH (b:Brand)-[r:Maker]->(p:Product)\n",
    "RETURN p.name AS s_no, COUNT(o) AS sold_most\n",
    "String category values:\n",
    "Use existing strings and values from the schema provided. \n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    "    template=cypher_generation_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_generation_template_str = \"\"\"\n",
    "You are an assistant that takes the results from a Neo4j Cypher query and forms a human-readable response. The query results section contains the results of a Cypher query that was generated based on a user's natural language question. The provided information is authoritative; you must never question it or use your internal knowledge to alter it. Make the answer sound like a response to the question.\n",
    "\n",
    "Query Results:\n",
    "{context}\n",
    "Question:\n",
    "{question}\n",
    "If the provided information is empty, respond by stating that you don't know the answer. Empty information is indicated by: []\n",
    "If the information is not empty, you must provide an answer using the results. If the question involves a time duration, assume the query results are in units of days unless specified otherwise.\n",
    "When names are provided in the query results, such as hospital names, be cautious of any names containing commas or other punctuation. For example, 'Jones, Brown and Murray' is a single hospital name, not multiple hospitals. Ensure that any list of names is presented clearly to avoid ambiguity and make the full names easily identifiable.\n",
    "Never state that you lack sufficient information if data is present in the query results. Always utilize the data provided.\n",
    "Helpful Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=qa_generation_template_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    allow_dangerous_requests=True,\n",
    "    top_k=10,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    validate_cypher=True,\n",
    "    qa_prompt=qa_generation_prompt,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    qa_llm=OllamaFunctions(model=\"llama3.1\", temperature=0),\n",
    "    cypher_llm=OllamaFunctions(model=\"llama3.1\", temperature=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the most expensive product?\"\n",
    "response = cypher_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.get(\"result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"List the top 5 products with the lowest price.\"\n",
    "response = cypher_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.get(\"result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give me details on Parx product price , description and images.\"\n",
    "response = cypher_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.get(\"result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_details_chat_template_str = \"\"\"\n",
    "Your job is to use the provided product data to answer \n",
    "questions about the description, brand, price and quality\n",
    "to the E-commerce customer. Use the following context to answer questions. \n",
    "Be as detailed as possible, but don't make up any information that's \n",
    "not from the context. \n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"product-qa-tool\", return_direct=False)\n",
    "def product_qa_tool(query: str) -> str:\n",
    "    \"\"\"Useful for answering questions about products made by the brands.\"\"\"\n",
    "    \n",
    "    product_details_chat_system_prompt = SystemMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=[\"context\"],\n",
    "            template=product_details_chat_template_str\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    human_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"Can you provide details on: {question}?\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    messages = [product_details_chat_system_prompt, human_prompt]\n",
    "    \n",
    "    qa_prompt = ChatPromptTemplate(\n",
    "        messages=messages,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    llm = OllamaFunctions(model=\"llama3.1\")\n",
    "\n",
    "    \n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=neo4j_graph_vector_index.as_retriever(),\n",
    "        # ['stuff', 'map_reduce', 'refine', 'map_rerank']\n",
    "        chain_type=\"stuff\",\n",
    "    )\n",
    "    \n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = qa_prompt\n",
    "    \n",
    "    response = qa_chain.invoke(query)\n",
    "    \n",
    "    return response.get(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_qa_tool.invoke(\"What is the price of Slim Fit Printed Casual Shirt?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"general-qa-tool\", return_direct=False)\n",
    "def general_qa_tool(query: str) -> str:\n",
    "    \"\"\"Useful for answering general questions about product, price, in_stock, gender, and currency.\"\"\"\n",
    "    response = cypher_chain.invoke(query)\n",
    "    \n",
    "    return response.get(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_qa_tool.invoke(\"What is the most expensive product?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import (\n",
    "    AgentExecutor,\n",
    "    create_openai_tools_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [product_qa_tool, general_qa_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaFunctions(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_openai_tools_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"What is the most expensive product?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"What is the price DYNK Shirt\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_details_chat_template_str = \"\"\"\n",
    "Your job is to use the provided product data to answer \n",
    "questions about the description, brand, price and quality\n",
    "to the E-commerce customer. Use the following context to answer questions. \n",
    "Be as detailed as possible, but don't make up any information that's \n",
    "not from the context. If you don't know an answer, say you don't know.\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"product-qa-tool\", return_direct=True)\n",
    "def product_qa_tool(query: str) -> str:\n",
    "    \"\"\"Useful for answering questions about products made by the company brands.\"\"\"\n",
    "    product_details_chat_system_prompt = SystemMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=[\"context\"],\n",
    "            template=product_details_chat_template_str\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    human_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"Can you provide details on: {question}?\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    messages = [product_details_chat_system_prompt, human_prompt]\n",
    "    \n",
    "    qa_prompt = ChatPromptTemplate(\n",
    "        messages=messages,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    llm = OllamaFunctions(model=\"llama3.1\")\n",
    "    \n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=neo4j_graph_vector_index.as_retriever(),\n",
    "        # ['stuff', 'map_reduce', 'refine', 'map_rerank']\n",
    "        chain_type=\"stuff\",\n",
    "    )\n",
    "    \n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = qa_prompt\n",
    "    \n",
    "    response = qa_chain.invoke(query)\n",
    "    \n",
    "    return response.get(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_qa_tool.invoke(\"What are the products from Parx brand?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"general-qa-tool\", return_direct=True)\n",
    "def general_qa_tool(query: str) -> str:\n",
    "    \"\"\"Useful for answering general questions about product name, price, brand, availability, description and link.\"\"\"\n",
    "    response = cypher_chain.invoke(query)\n",
    "    \n",
    "    return response.get(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_qa_tool.invoke(\"What is the most made product from branded company in the database?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import (\n",
    "    AgentExecutor,\n",
    "    create_tool_calling_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [product_qa_tool, general_qa_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaFunctions(model=\"llama3.1\",format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_openai_tools_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the price of DYNK Shirt?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install gpt4all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
